<!-- GNU TERRY PRATCHETT -->

<!DOCTYPE html>
<html lang="fr">

<head>
  <meta charset="utf-8" />
  <title>NER-Annotateur-Quenya</title>
  <link rel="stylesheet" href="style.css">
</head>

<body>
<!-- Navbar à gauche -->
  <div class="navbar">
      <a href="quenya.html">Extraction NER</a>
      <a href="pipeline_spacy.html">Pipepline Spacy</a>
      <a href="blstm_crf.html">BSLTM CRF</a>
      <a href="methodologie.html">Méthodologie</a>
      <a href="sources.html">Sources</a>
  </div>

<!--Conteneur principal -->
  <div class="main-content">
    <div class="blurred-content">

      <h1 border= 5px solid #3498db >Pipeline Spacy</h1>

      <p>
        La première méthode utilisée pour la création d'un annotateur NER pour le Quenya est celle de l'architecture proposée par Spacy.
      </p>
      <p>
        Spacy est une bibliothèque Python dédiée au traitement automatique des langues, proposant divers outils pour des tâches telles que le parsing, la reconnaissance d'entités nommées et le POS-tagging.
        Bien que Spacy couvre plusieurs langues, il ne prend malheureusement pas en charge l'elfique (pour l'instant...) !
      </p>

      <h2>Architecture NER</h2>
      <p>
        L'architecture NER de Spacy repose sur un système de transition de séquences. Un token peut être identifié comme le début d'une entité, une partie d'une entité ou hors de toute entité.
        Ces différents états permettent au modèle de reconnaître les entités en analysant leur contexte et comprendre quelles caractéristiques les définissent.
      </p>
        <p>
            Spacy propose une pipeline complète intégrant des outils de reconnaissance pré-entraînés, utilisables directement ou comme baseline pour des applications spécifiques.
            Cette pipeline inclut plusieurs étapes :
        </p>
        <ul>
            <li><strong>Tokenizer :</strong> Divers modèles adaptés à plusieurs langues.</li>
            <li><strong>Vectorizer :</strong> Modèles de vectorisation pour représenter chaque token.</li>
            <li><strong>Transition Based Parser :</strong> Reconnaissance des entités.</li>
        </ul>

        <h2>Utilisation</h2>
        <h3>Formatage des données</h3>
        <p>
            Spacy nécessite un format spécifique pour l'entraînement et la prédiction des entités nommées. 
            Les annotations doivent être stockées dans un fichier JSON sous la forme d'une liste contenant autant de dictionnaires que de textes dans le corpus.
            Chaque dictionnaire contient les items suivants :
        </p>
        <ul>
            <li><strong>'text'</strong> : le texte complet.</li>
            <li><strong>'entities'</strong> : une liste de dictionnaires contenant le type d'entité, l'index de début et l'index de fin.</li>
        </ul>

        <center><img src="img/spacy_input.png"/></center>

        <p>
            Ce fichier JSON doit être converti en un format exploitable par Spacy à l'aide du script <code>conversion.py</code>. 
            Cette conversion peut engendrer des erreurs, notamment au niveau d'entités superposées (ex: un nom pouvant être à la fois une PERSON et un LOC). 
            Des corrections manuelles ont été apportées pour minimiser ces erreurs.

        </p>

        <h3>Entraînement</h3>
            <p>
                Lors du premier lancement de la pipeline NER, Spacy génère automatiquement un fichier <strong>config</strong> contenant les détails des modèles et étapes du traitement :
            </p>
            <ul>
                <li><strong>Tokenizer :</strong> <code>Spacy.Tokenizer.v1</code></li>
                <li><strong>Vectorisation :</strong> <code>Spacy.Vectors.v1</code> (ou <code>Tok2Vec</code> si aucun embedding n'est fourni).</li>
                <li><strong>Encodage des tokens :</strong> <code>Spacy.MultiHashEmbed.v2</code>.</li>
                <li><strong>Reconnaissance d'entités :</strong> Transition-Based Parser (Multi Layer Perceptron + contexte token précédent + état actuel).</li>
            </ul>
            <p>
                Le label 'xx' permet d'imposer l'utilisation de modèles multilingues afin de s'adapter au mieux à une langue non couverte par la librairie.
                Spacy commence par tokéniser le texte donné en input puis le vectorise à l'aide d'un processus impliquant un réseau de neurones convolutif. <code>Tok2Vec</code> permet de fournir pour chaque token
                un embedding représentatif de son contexte, you shall know a word by the company it keeps. <code>MultiHashEmbed</code> permet lui d'encoder des attributs de forme pour chaque token comme d'éventuels préfixes ou suffixes.
                Ces encodages sont ensuite soumis à une couche d'activation MaxOut permettant de renvoyer un embedding final. Le <code>Transition-Based Parser</code> prend ensuite la représentation vectorielle d'un token en entré. En se basant sur cette dernière
                ainsi que sur les prédictions précédentes, il établit une probabilité de transition (donc début, fin, et type d'entité) pour le token correspondant à l'embedding. Cette suite de prédictions compose l'annotation finale proposée par le modèle.

            </p>

            <h3>Changement des hyperparamètres</h3>
            <p>
                Le fichier de configuration contient également les hyperparamètres du modèle. Nous avons expérimenté plusieurs combinaisons pour optimiser la reconnaissance des entités.
                Les valeurs modifiées incluent :
            </p>
            <ul>
                <li><strong>upper_use :</strong> Utilisation de la casse pour la reconnaissance d'entités (<code>True</code>, <code>False</code>).</li>
                <li><strong>learn_rate :</strong> Valeur de la learning rate (<code>0.01</code>, <code>0.001</code>).</li>
                <li><strong>batch_size :</strong> Nombre d'échantillons traités simultanément lors de l'évaluation (<code>100</code>, <code>500</code>, <code>1000</code>).</li>
                <li><strong>rate :</strong> Nombre d'échantillons traités simultanément lors de l'entraînement (<code>100</code>, <code>500</code>, <code>1000</code>).</li>
            </ul>

            Des entraînements supplémentaires ont de plus été effectués en utilisant les modèles de tokénisation et de vectorisation du finnois.
            
            <h3>Évaluation</h3>

            <center><img src="img/training_pipeline.png"/></center>
            <p>
                <p>
                    L'output de la pipeline d'entraînement permet d'avoir accès aux métriques d'évaluation calculées pour chaque epoch. 
                    Des métriques plus détaillées (pour chaque classe) sont disponibles dans les dossiers créés après chaque entraînement.
                </p>
                
                <p>
                    Les résultats obtenus sur l'ensemble de validation nous ont permis d'observer certaines tendances :
                </p>
                
                <ul>
                    <li>
                        Les scores obtenus pour les entités de type <strong>'PERSON'</strong> sont généralement plus élevés que pour les entités de type <strong>'LOC'</strong>. 
                        Nous pensons que cela est dû à une plus grande variabilité des noms de personnages dans les textes de Tolkien. 
                    </li>
                    
                    <li>
                        Les scores de précision semblent varier de manière plus importante que les scores de rappel. 
                        Le modèle semble donc performer de manière plus stable sur la reconnaissance d'entités que sur leur classification. 
                        Les entités sont reconnues de manière homogène entre chaque epoch, mais le nombre de faux positifs varie plus fortement.
                    </li>
                    
                    <li>
                        La F-Mesure, bien qu'elle puisse osciller de manière importante entre chaque epoch pour certains entraînements, reste constamment plus élevée que la moyenne. 
                        Il semblerait que le modèle soit capable de reconnaître les entités de manière satisfaisante, même si la classe majoritaire (PERSON) peut entraîner des mauvaises classifications.
                    </li>
                </ul>
                
                <p>
                    Nous pouvions désormais vérifier ces tendances sur l'ensemble de test. 
                </p>
                
                <p>
                    La F-mesure moyenne pour les 24 essais est de <strong>0.628</strong>. 
                    Lorsque nous faisons la moyenne de la F-score sur les modèles finlandais et multilingues, nous obtenons respectivement <strong>0.623</strong> et <strong>0.634</strong>. 
                    La différence semble minime, mais montre bien que le finlandais est particulièrement adapté à la langue quenya.
                </p>
                
                <p>
                    Indépendamment des modèles de tokenisation et de vectorisation utilisés, voici maintenant les résultats obtenus pour les 3 combinaisons d'hyperparamètres les plus performantes :
                </p>
            <table>
                <table border="1">
                <caption><strong>Résultats des 3 meilleurs modèles</strong></caption>
                <thead>
                    <tr>
                        <th>Langue du modèle</th>
                        <th>Hyperparamètres</th>
                        <th>F-Mesure</th>
                        <th>Rappel</th>
                        <th>Précision</th>
                        <th>F-Mesure PERSON</th>
                        <th>F-Mesure LOC</th>
                    </tr>
                </thead>
                <tbody>
               
                    <tr>
                        <td>Finnish</td>
                        <td><code>upper_use: False, learning_rate: 0.01, batch_size: 1000</code></td>
                        <td><strong>0.684</strong></td>
                        <td>0.619</td>
                        <td>0.764</td>
                        <td>0.783</td>
                        <td>0.375</td>
                    </tr>
                    <tr>
                        <td>Multilingual</td>
                        <td><code>upper_use: False, learning_rate: 0.001, batch_size: 1000</code></td>
                        <td><strong>0.682</strong></td>
                        <td>0.69</td>
                        <td>0.674</td>
                        <td>0.753</td>
                        <td>0.375</td>
                    </tr>
                    <tr>
                        <td>Multilingual</td>
                        <td><code>upper_use: False, learning_rate: 0.001, batch_size: 100</code></td>
                        <td><strong>0.679</strong></td>
                        <td>0.642</td>
                        <td>0.72</td>
                        <td>0.76</td>
                        <td>0.421</td>
                    </tr>
                </tbody>
            </table>            
          
            <p>
                Ces résultats peuvent premièrement nous montrer que c'est en ne prenant <strong>pas en compte la casse</strong> que nous obtenons de meilleurs résultats. 
                Bien que cela puisse sembler contre-intuitif au départ, cela pourrait s'expliquer par l'utilisation de majuscules dans des constructions spécifiques des poèmes du corpus. 
                Néanmoins, les modèles utilisant la casse affichent des scores relativement proches de ceux mentionnés ci-dessus.            </p>
            
            <p>
                En ce qui concerne les autres hyperparamètres, il semble qu'un <strong>batch size</strong> plus grand améliore généralement les résultats. 
                Cela indique qu'une prise en compte plus large du contexte est cruciale dans le cadre de notre projet et au vu des données utilisées.             </p>
            
            <p>
                Il est important de rappeler que ces résultats concernent uniquement une seule combinaison d'ensembles de train, test et validation. 
                Après plusieurs essais sur ces ensembles, les résultats obtenus demeurent très similaires, ce qui indique que nos modèles apprennent de manière constante. 
                Cependant, lorsque nous effectuons les mêmes comparaisons avec des splits différents (obtenus aléatoirement), nous pouvons obtenir des résultats considérablement variés.
            </p>
            
            <p>
                Par exemple, un split a permis d'obtenir un score de F-Mesure globale de <strong>0.955</strong> (<code>secondtrue_5_1600.json</code>), avec <strong>0.972</strong> pour PERSON et <strong>0.936</strong> pour LOC. 
                D'autres essais ont donné des résultats plus bas, mais une grande partie a également produit des résultats proches de ceux obtenus avec notre split "équilibré". 
                Bien qu'une performance "moyenne" d'environ <strong>60%</strong> se retrouve dans plusieurs partitions, la qualité des résultats dépend énormément des données utilisées pour l'entraînement et l'évaluation. 
            </p>
            
            <p>
                Nous souhaitions ensuite utiliser le modèle le plus performant pour annoter trois textes qui ne proviennent pas des données d'entraînement, de test ou de validation. 
                Le premier texte est écrit par Tolkien, le second par un fan en néo-quenya, et le dernier est une traduction de la bible (également en néo-quenya). 
                Vous pouvez tester le modèle vous-même à l'aide du script <code>illustration.py</code>.
            </p>
            
            <h4>Texte 1 - Tolkien : <em>Eldar ataformaiti</em></h4>
            <p>Les entités annotées dans le texte correspondent aux entités <i>attendues</i> ("correctes"). Les résultats obtenus sont eux présentés dans les screenshots.</p>

            <p> <span class="person">PERSON</span> &nbsp;&nbsp;&nbsp;&nbsp; <span class="location">LOC</span></p>

            <blockquote>
                <em><b>
                Eldar ataformaiti, epetai i hyarma ú ten ulca símaryassen.
                úsië, an cé mo quernë cendelë númenna, ve senya, i hyarma tentanë <span class="person">Melcorello</span>, ar cé mo formenna tentanes <span class="location">Amanna</span>.
                </b></em>
            </blockquote>

            <blockquote>
                <em>
                The Elves were ambidexters, consequently the left hand was not to them evil in their imaginations.
                On the contrary, for if one turned the face westward, as was usual, the left hand pointed away from Melkor, and if northwards, it pointed toward towards Aman.
                </em>
            </blockquote>

                        <div class="image-container", style="text-align:center">
                <img src="img/neo_quenya_results_spacy.png" alt="NER Results for Gospel">
            </div>

            <p>
            Pour cet exemple ci, le modèle repère "Eldar", signifiant "Elfes" en quenya. Il ne s'agit ici pas d'un lieu comme le pense le modèle. De même, si "Amanna" est bel et bien reconnu, il est classé comme une personne alors qu'il s'agit d'un lieu. "Melcorello" n'est lui pas reconnu du tout. Nous ne pouvons ici pas vraiment savoir si "Melkor" fait référence à "Melkor" (le grand pas gentil) ou à un lieu le personnifiant.
            Les résultats plus faibles pourraient donc ici découler d'une interprétation plus libre des types d'entités présentes.
            </p>

            <h4>Texte 2 - Néo-Quenya : <em>Kelevala</em></h4>

            <blockquote>
                <em><b>
                San sinë Tarcildillon i etelehtaner cuilintar nórentava lantallo, Rómenna utúlier vë quenta ná mí Akallabeth.
                I canonta né <span class="person">Elendil Halla</span> ar yondoryat - <span class="person">Isildur</span> ar <span class="person">Anárion</span> - orórot Arfarazonwa ar neurot Elrossevai alalastaner <span class="person">Sauron</span> ar alaohtacarer Herunúmennar.
                Avánier Atalanter <span class="location">Númenórello</span> entë ar ilyë Tarcildi vorondar ciryainen.
                Sinë neri ner meletyë ar haryaner ciryar poldë ar hallë, nán raumo lantanë entennar ar amortanet nenamboinen tenna i lumbar, ar taltanentë  falassenna Endorwa vë sornor raumova.
                </b></em>
            </blockquote>

            <blockquote>
                <em>
                In that time those of the Númenóreans who were saved from destruction fled eastward,
                as is told in the Akallabêth. The chief of these were Elendil the Tall and his sons, Isildur and Anárion.
                Kinsmen of the King they were, descendants of Elros, but they had been unwilling to listen to Sauron, and had refused to make war on the Lords of the West.
                Manning their ships with all who remained faithful they forsook the land of Númenor ere ruin came upon it.
                They were mighty men and their ships were strong and tall, but the tempests overtook them, and they were borne aloft on hills of water even to the clouds, and they descended upon Middle-earth like birds of the storm.
            </blockquote>
                </em>

            <div class="image-container", style="text-align:center">
                <img src="img/neo_quenya_bible_results_spacy.png" alt="NER Results for Tolkien Text">
            </div>

            <p>
            "Elendil" et "Sauron" sont tous les deux reconnus, mais classifiés à tort comme des lieux (il s'agit de personnages).
            En revanche, "Númenórello" (ablatif de Númenor) est correctement reconnu comme un lieu, et ce alors qu'il n'est pas présent dans les données d'entraînement (en tout cas, pas avec ces accents hehe woop woop).
            Bien sûr, d'autres entités non présentes dans nos listes sont reconnues dans l'ensemble de test.
            </p>



            <h4>Texte 3 - Néo-Quenya : <em>L'évangile selon Matthieu</em></h4>

             <blockquote>
                <em><b>
                Si <span class="person">Yoháno</span> vettie ná íre i Yúrar mentaner airimóli ar Levindeli <span class="location">Yerúsalemello</span> maquetien senna:
                “Man nalye?” Ar carampes pantave ar ua lalane, mal quente pantave: “Uan i <span class="person">Hristo</span>.” Ar maquentelte senna:
                “Tá mana? Ma nalye <span class="person">Elía</span>?” Ar eques: “Uan.” “Ma nalye i Erutercáno?” Ar hanquentes: “Lá!” Etta quentelte senna:
                “Man nalye? Lava men same hanquenta in mentaner me. Mana quetil pa imle?” Eques: “Nanye óma yamila i ravandasse:
                Cara i <span class="person">Héruo</span> malle téra! – tambe <span class="person">Yesaia i Erutercáno</span> quente.”
                </b></em>
            </blockquote>
            <blockquote>
                <em>
                This is the witness of John when the Jews sent some priests and Levites from Jerusalem to inquire of him:
                “Who are you?” And he spoke openly and did not deny [it], but said openly: “I am not the Christ.”
                And they asked him: “Then what? Are you Elijah?” And he said: “I am not.” “Are you the Prophet?” And he answered:
                “No!” Therefore they said to him: “Who are you? Let us have an answer for those who sent us. What do you say about yourself?”
                He said: “I am a voice crying in the desert: Make the Lord’s way straight! – as Isaiah the Prophet said.”
                </em>
            </blockquote>

            <div class="image-container", style="text-align:center">
                <img src="img/tolkien_results_spacy.png" alt="NER Results for Kelevala">
            </div>

            <p>
            "Hristo"(nominatif), "Yoháno"(nominatif) et "Héruo"(génitif) sont correctement reconnus comme des personnes. "Erutercáno" n'est pas reconnu comme un nom de personnage (et heureusement !), signifiant "le prophète" en quenya. "Yesaia i Erutercáno" est lui bien reconnu comme tel !
            Le modèle a plus de mal avec les noms de lieux. "Yerúsalemello (Jérusalem)" n'est pas du tout reconnu malgré deux occurrences.
            Il est donc peut-être possible que le modèle ait du mal à reconnaître les noms de lieux avec cas, "Yerúsalemello" étant la forme ablatif de "Yerúsalem" en quenya.
            </p>



            <p>
            En conclusion, le modèle semble être assez performant pour reconnaître les entités mais un peu moins pour les classifier. 
            </p>
  </div>
  </div>
</body>


</html>

